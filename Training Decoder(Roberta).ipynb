{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM # RobertaLM for learning\n",
    "from transformers import RobertaTokenizerFast # After training tokenizer we will wrap it so it can be used by Roberta model\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIRECTORY = \"./data/Images\"\n",
    "TOKENIZER_DIRECTORY = \"./models/Byte_tokenizer\"\n",
    "ROBERTA_DIRECTORY = \"./models/RobertaDecoder\"\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64   # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 64   # input batch size for testing (default: 1000)\n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "MAX_LEN = 128           # Max length for caption\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "TRAIN_EPOCHS = 10       # number of epochs to train (default: 10)\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afb71f",
   "metadata": {},
   "source": [
    "# Training Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3b08a",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this notebook we will train a decoder for our very own image captioning model. Also, we will cover the most important concepts required to understand how is our image captioning model working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70402e42",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e7e6c",
   "metadata": {},
   "source": [
    "### Image Captioning\n",
    "\n",
    "Image captioning is as an End-to-End Sequence to Sequence embedding task where image pixels are input sequences and caption describing the image is desired output.\n",
    "\n",
    "Due to exclusive nature of both images and text sequences two different model tied together (one dedicated to Encode from images and other Decode to a text Sequence) are required to solve this task.\n",
    "\n",
    "This idea is synonymous to traditional Encoders & Decoders used to resolve processing and propagation of electronic signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607107e9",
   "metadata": {},
   "source": [
    "### Encoder-Decoder\n",
    "\n",
    "Encoder-decoder is a type of neural network architecture that is commonly used for image captioning. The basic idea behind this architecture is to first \"encode\" an image into a fixed length representation, which captures the main features and characteristics of the image. This encoded representation is then passed through a \"decoder\" network, which generates a caption for the image.\n",
    "\n",
    "The encoder network typically uses convolutional neural networks (CNNs) to extract features from the image. The decoder network typically uses recurrent neural networks (RNNs) to generate the caption, as it needs to consider the context of the previous words in the sequence when generating the next word in the caption.\n",
    "\n",
    "In simple terms, the encoder-decoder network takes an image as input, transforms it into a compact representation using the encoder, and then generates a descriptive caption using the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ccb91",
   "metadata": {},
   "source": [
    "#### 1-Encoder\n",
    "\n",
    "The encoder in image captioning is a deep neural network that takes an image as input and generates a dense fixed-length representation, or encoding, of the image. The purpose of the encoder is to extract and compress meaningful information from the image and represent it in a compact form that can be used by the decoder to generate a caption. The encoder typically consists of several convolutional and pooling layers that are designed to capture the hierarchical structure and spatial relationships of the objects and features in the image. The output of the encoder is usually a feature map or a set of feature vectors that encapsulate the important information about the image.\n",
    "\n",
    "Here we will use Vision Transformer (ViT) as encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf66db7",
   "metadata": {},
   "source": [
    "#### 2-Decoder\n",
    "\n",
    "The decoder for image captioning is responsible for generating text captions based on the encoded information from the image. It typically uses an attention mechanism that allows the decoder to focus on different parts of the encoded image at different times while generating the caption. The decoder processes the encoded information and produces a sequence of words that describe the content of the image. The decoder uses the encoded information as a context vector, and at each step of the caption generation process, it predicts the next word in the caption based on this context vector and the previously generated words. The decoder generates the final caption word by word until it produces a complete sentence that describes the image. In our model the decoder will be Roberta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a83c2",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Attention is a mechanism in deep learning models that allows the model to focus on specific parts of the input, rather than processing all of it equally. It works by assigning importance weights to different elements of the input, which are then used to compute the final representation of the input.\n",
    "\n",
    "The attention mechanism can be represented mathematically as a dot product operation between a query vector and a set of key-value pairs, followed by a softmax function. The formula is as follows:\n",
    "\n",
    "$$Attention(Q, K, V) = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "$$= Softmax(\\frac{QK^T}{\\sqrt{d_k}}) \\cdot V$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $Q$: The query matrix is used to represent the current state or context in which the model is trying to make a prediction. It is often a learned representation of the input.\n",
    "\n",
    "* $K$: The key matrix represents the elements in the input that the model wants to attend to. The dot product between the query and key matrices determines the attention weights for each key.\n",
    "\n",
    "* $V$: The value matrix represents the information that is associated with each key. The attention weights are used to compute a weighted sum of the values, which is used as the output of the attention mechanism.\n",
    "\n",
    "* $d_k$: The dimension of the keys determines the size of the dot product between the query and key matrices. It is used as a scaling factor in the attention mechanism to prevent the dot product from becoming too large.\n",
    "\n",
    "* $\\cdot$: The dot product is used to compute the similarity between the query and key matrices. The result of the dot product is used to compute the attention weights.\n",
    "\n",
    "* $K^T$: The transpose of the key matrix is used in the dot product operation. Taking the transpose allows the dot product to be computed between the columns of the query matrix and the rows of the key matrix.\n",
    "\n",
    "* $\\frac{QK^T}{\\sqrt{d_k}}$: This expression computes the dot product between the query and key matrices, scaled by the square root of the dimension of the keys. The result of this operation is used as the input to the softmax function to compute the attention weights.\n",
    "\n",
    "The attention mechanism allows the model to focus on the most relevant parts of the input, which helps improve its performance on various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fd39c",
   "metadata": {},
   "source": [
    "#### Cross-Attention\n",
    "\n",
    "Cross-attention refers to a type of attention mechanism where the attention scores are computed between the queries and keys from two different inputs.\n",
    "\n",
    "For example, in a machine translation task, the queries may come from the source language and the keys may come from the target language. The attention scores computed between the two inputs determine how much attention the model should pay to each word in the target language when predicting the words in the source language. This allows the model to focus on the most relevant information from both inputs when making its predictions.\n",
    "\n",
    "In this way, cross-attention allows the model to capture complex relationships between the inputs, leading to more accurate predictions and improved performance on various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff916d",
   "metadata": {},
   "source": [
    "#### Self-Attention\n",
    "\n",
    "Self-attention is a type of attention mechanism where the queries, keys, and values all come from the same input. The model computes attention scores between the elements of the input to determine how much importance to assign to each element when making a prediction.\n",
    "\n",
    "For example, in a language modeling task, the input might be a sequence of words and the self-attention mechanism might compute attention scores between each pair of words in the sequence. The attention scores can then be used to determine the most relevant words to consider when making a prediction for the next word in the sequence.\n",
    "\n",
    "By computing attention scores between elements of the same input, self-attention allows the model to capture dependencies and relationships between elements of the input in a direct and intuitive way, leading to improved performance on various tasks. The main difference between Self- and Cross-Attention is the source of the queries, keys, and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5d4a9",
   "metadata": {},
   "source": [
    "### RoBERTa\n",
    "\n",
    "RoBERTa (Robustly Optimized BERT Approach) is a state-of-the-art language model developed by Facebook AI Research. It is based on the popular BERT (Bidirectional Encoder Representations from Transformers) architecture and uses transformer networks to process and generate text data.\n",
    "\n",
    "In simple terms, RoBERTa is a machine learning model that has been trained on a large corpus of text data to understand the patterns and relationships between words and sentences in a language. It can then be used for various NLP tasks such as text classification, question answering, and text generation.\n",
    "\n",
    "RoBERTa's training procedure is optimized to better handle the challenges of NLP, such as handling out-of-vocabulary words, and its large size allows it to effectively capture the fine-grained details of text data. This makes RoBERTa one of the most powerful language models available and it is widely used in NLP research and industry applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe6145",
   "metadata": {},
   "source": [
    "## Preparing the Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad04a38",
   "metadata": {},
   "source": [
    "Firstly, we will load our captions from json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/captions.json', 'r') as openfile:\n",
    "    caption_dict = json.load(openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1985a2",
   "metadata": {},
   "source": [
    "Next, we will get the paths of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in list(caption_dict.keys()):\n",
    "    if image_path.endswith('jpg'):\n",
    "        image_path = image_path.replace(\"ImagesImages\", \"\")\n",
    "        new = IMAGES_DIRECTORY + image_path.split('/')[-1]\n",
    "        caption_dict[new] = caption_dict.pop(image_path)\n",
    "    else:\n",
    "        caption_dict.pop(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90914d",
   "metadata": {},
   "source": [
    "After that we will get our dataframe with images and captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([])\n",
    "\n",
    "captions = []\n",
    "images = []\n",
    "for image in list(caption_dict.keys()):\n",
    "    captions_for_image = caption_dict[image]\n",
    "    for caption in captions_for_image:\n",
    "        captions.append(caption.replace('<s> ','').replace('  <e>','').strip())\n",
    "        images.append(image)\n",
    "        \n",
    "df['images'] = images\n",
    "df['captions'] = captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685195c",
   "metadata": {},
   "source": [
    "Here is how our DataFrame with paths and captions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ab7d3",
   "metadata": {},
   "source": [
    "### Training the Decoder Model for Language Understanding and build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80336e89",
   "metadata": {},
   "source": [
    "In this section we will train our RoBERTa decoder and create a tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12a0c9",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f884d29",
   "metadata": {},
   "source": [
    "Here we will train a byte-level tokenizer.\n",
    "\n",
    "Byte-level tokenization is a method of splitting a string of text into individual tokens, where each token is defined as a sequence of contiguous bytes. In this method, the tokenization process does not consider the structure or meaning of the text, but rather focuses solely on the individual bytes and their sequences.\n",
    "\n",
    "This type of tokenization can be useful in certain applications, such as processing text in non-Latin scripts, where words are not separated by spaces and traditional word-level tokenization methods may not work well. By using byte-level tokenization, the model can still work with the text, even if it doesn't have a complete understanding of the language.\n",
    "\n",
    "In summary, byte-level tokenization is a simple and flexible method of splitting text into individual tokens, where each token is defined as a sequence of bytes, regardless of the structure or meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb069c7",
   "metadata": {},
   "source": [
    "#### Converting captions in to .txt file for training of the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bc0e9",
   "metadata": {},
   "source": [
    "The Byte-Level BPE Tokenizer requires each text to be in a separate file because it is designed to work with text data that has already been preprocessed into individual files. This is a common format for text data, as it allows for easy processing and management of the data. Additionally, having each text in a separate file makes it easier to work with the data in a parallelizable way, which can be important when dealing with large amounts of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51818253",
   "metadata": {},
   "source": [
    "Firstly, we will create a directory for the txt files, if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/text_split\"):\n",
    "    os.mkdir(\"./data/text_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed63af0",
   "metadata": {},
   "source": [
    "The, we will define function that writes caption to each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97745970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_to_files(column, prefix, txt_files_dir = \"./data/text_split\"):\n",
    "    # The prefix is a unique ID to avoid to overwrite a text file\n",
    "    i=prefix\n",
    "    #For every value in the df, with just one column\n",
    "    for row in column.to_list():\n",
    "      # Create the filename using the prefix ID\n",
    "        file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
    "        try:\n",
    "            # Create the file and write the column text to it\n",
    "            f = open(file_name, 'wb')\n",
    "            f.write(row.encode('utf-8'))\n",
    "            f.close()\n",
    "        except Exception as e:  #catch exceptions(for eg. empty rows)\n",
    "            print(row, e) \n",
    "        i+=1\n",
    "    # Return the last ID\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd5dcc",
   "metadata": {},
   "source": [
    "The, we will write the captions to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_column = df[\"captions\"]\n",
    "captions_column = captions_column.replace(\"\\n\",\" \")\n",
    "\n",
    "prefix=0\n",
    "\n",
    "# the function returns the last prefix, so we can be sure that there are no overwritten files\n",
    "prefix = captions_to_files(captions_column, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc069cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prefix == len(captions_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55eaff",
   "metadata": {},
   "source": [
    "#### Training tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98fe10",
   "metadata": {},
   "source": [
    "Now, we can create and train our tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff1420",
   "metadata": {},
   "source": [
    "Firstly, we will get all the paths of the files we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splits_paths = [str(x) for x in Path(\".\").glob(\"./data/text_split/*.txt\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88783e66",
   "metadata": {},
   "source": [
    "Then, we will initialize our tokenizer with parameter `lowercase` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cad0c2",
   "metadata": {},
   "source": [
    "Now, we will train our tokenizer (this may take a few minutes), but first we will see what are those parameters we pass to the `train` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c56962",
   "metadata": {},
   "source": [
    "* 1. vocab_size - This parameter determines the maximum number of words the tokenizer will know.\n",
    "\n",
    "* 2. min_frequency - This parameter determines the minimum frequency of the subwords in vocabulary. This parameter is useful because if there are subwords that are seen only once (referred to as \"out-of-vocabulary\" or \"OOV\" words) are not helpful because they will not be recognized by the model's vocabulary.  When a model encounters an OOV word, it must either ignore it or map it to a generic token, such as an \"unknown\" token, which may not accurately capture the meaning of the original word. In contrast, sub-word units that are seen multiple times in the training data are more likely to be meaningful and to capture important word structures in the text, making them more useful for encoding text data.\n",
    "\n",
    "* 3. special_tokens - They are specific string tokens that have a special meaning or function in a language model. They are used to perform tasks such as separating text into sentences or marking the beginning and end of a sequence. These special tokens play a crucial role in allowing NLP models to effectively process and generate text data. Some common special tokens include:\n",
    "\n",
    "    * [SEP] (separation token): Used to separate different text sequences within a single input, such as when encoding multiple sentences in a single input.\n",
    "    * [PAD] (padding token): Used to pad sequences to a fixed length during training and inference.\n",
    "    * [MASK] (masking token): Used to hide a word for language modeling tasks such as mask language modeling.\n",
    "    * [UNK] (unknown token): Used to represent out-of-vocabulary words that are not present in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b01bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tokenizer.train(files=text_splits_paths, vocab_size=VOCAB_SIZE, min_frequency=2,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"<e>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac200d",
   "metadata": {},
   "source": [
    "After we trained the tokenizer, we can see what it vocabulary consists of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenizer.vocab.keys())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23567210",
   "metadata": {},
   "source": [
    "In the vocabulary sample, the sub-word units have been generated such that common word prefixes and suffixes are merged into a single sub-word unit. For example, \"corridor\" and \"rummaged\" both have the common prefix \"Ġ\" (which may be represented by a special character), indicating that the prefix \"cor\" and \"rum\" have been merged into a single sub-word unit. This can help to reduce the size of the vocabulary and capture common patterns in the text data.\n",
    "\n",
    "Note that some sub-word units may not have a special character prefix or suffix, such as \"sing\" and \"ink\", indicating that these words are represented as individual sub-word units in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599851c",
   "metadata": {},
   "source": [
    "#### Save Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80311f32",
   "metadata": {},
   "source": [
    "Here, we do two simple things:\n",
    "\n",
    "* 1. Create a directory for the tokenizer and\n",
    "* 2. Save the tokenizer in the new directory\n",
    "\n",
    "If you wany, go to the directory of the tokenizer and check what is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463597cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(TOKENIZER_DIRECTORY)\n",
    "tokenizer.save_model(TOKENIZER_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c98310",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb235fc3",
   "metadata": {},
   "source": [
    "#### Creating the required objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44836b02",
   "metadata": {},
   "source": [
    "Since the model will not tokenize and train like magic by just giving it files like we did with the tokenizer. That is why we need to define a class extending PyTorch Dataset class in which we tokenize our captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.examples = []\n",
    "        \n",
    "        for example in df.values:\n",
    "            x=tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
    "            self.examples.append(x.input_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])\n",
    "\n",
    "# Create the train and evaluation dataset\n",
    "train_dataset = CustomDataset(captions_column[:38000], tokenizer)\n",
    "eval_dataset = CustomDataset(captions_column[38000:], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7eda5",
   "metadata": {},
   "source": [
    "After that we need to define our data collator. Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset . To be able to build batches, data collators may apply some processing (like padding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19f23c",
   "metadata": {},
   "source": [
    "The `mlm_probability` parameter defines the probability with which to randomly mask tokens in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27397f0",
   "metadata": {},
   "source": [
    "#### Intialization & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c625407",
   "metadata": {},
   "source": [
    "Now, we will create our model. First, we need to define its configuration:\n",
    "\n",
    "* vocab_size: The same parameter we passed to the tokenizer.\n",
    "* max_position_embeddings: The maximum sequence length that this model might ever be used with.\n",
    "* num_attention_heads: Number of attention heads. The larger the number, the more training time is required.\n",
    "* num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
    "* type_vocab_size: The vocabulary size of the `token_type_ids` passed when calling RobertaModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa23720",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=2048,\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437aa2f",
   "metadata": {},
   "source": [
    "Now, we will define our model and see how many parameters it has. Keep in mind that model with large number of parameters is not working for all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e9ba3",
   "metadata": {},
   "source": [
    "#### Training the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c170350",
   "metadata": {},
   "source": [
    "Firstly, we need to define the configuration for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bb452",
   "metadata": {},
   "source": [
    "We have the following parameters:\n",
    "\n",
    "* evaluation_strategy: This parameter defines how often to evaluate the model. In our case it will evaluate on full pass over dataset.\n",
    "\n",
    "* learning_rate: The learning rate is used to govern the pace at which an algorithm updates or learns the values of a parameter estimate. In other words, the learning rate regulates the weights of our model concerning the loss gradient.\n",
    "\n",
    "* weight_decay: Weight Decay, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the Norm of the weights\n",
    "\n",
    "* per_device_train_batch_size and per_device_eval_batch_size: Here we define the number of samples in a batch.\n",
    "\n",
    "* save_steps: Number of updates steps before two checkpoint saves.\n",
    "\n",
    "* save_total_limit: If a value is passed, will limit the total amount of checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94692682",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=ROBERTA_DIRECTORY,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    save_steps=8192,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf80671",
   "metadata": {},
   "source": [
    "Now we can initialize our `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd482eb5",
   "metadata": {},
   "source": [
    "And we train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc692d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef5286",
   "metadata": {},
   "source": [
    "### Evaluation and saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fe382",
   "metadata": {},
   "source": [
    "### Check Perplexity score of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8a9b3",
   "metadata": {},
   "source": [
    "Perplexity is a measure of how well a language model predicts a sequence of words. It calculates the likelihood of a given sentence and normalizes it by the number of words, to give a score that reflects the model's uncertainty. Lower perplexity values indicate that the model is better at predicting the text, whereas higher perplexity values indicate that the model is less confident in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01728b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed68da",
   "metadata": {},
   "source": [
    "### Saving tokenizer & Model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ad34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(TOKENIZER_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f26ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(ROBERTA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830a53d",
   "metadata": {},
   "source": [
    "### Testing our decoder on a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9d9d2",
   "metadata": {},
   "source": [
    "First, we load the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2efa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model= ROBERTA_DIRECTORY,\n",
    "    tokenizer= TOKENIZER_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874d76d",
   "metadata": {},
   "source": [
    "Then we use it to predict masked token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"a girl going to a <mask> building\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c2f31",
   "metadata": {},
   "source": [
    "As you can see, the model gives us pretty good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f443f3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we trained a RoBERTa decoder, learned a bunch of new things (probably) and saw how to use HuggingFace. In the next notebook we will use our model as decoder in our image captioning model and it will be connnected to ViT encoder model using cross attention heads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
