{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "from transformers import default_data_collator\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-2\n",
    "SEED = 42\n",
    "MAX_LEN = 128\n",
    "\n",
    "TRAIN_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ae4e6",
   "metadata": {},
   "source": [
    "# Training Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d547f45",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e127bd",
   "metadata": {},
   "source": [
    "In this notebook we will use the decoder from the previous notebook to train our image captioning model. But first we will take a look at some concepts that we did not discuss in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0f36c",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce773b",
   "metadata": {},
   "source": [
    "### Vision Transformer (ViT)\n",
    "\n",
    "A Vision Transformer is a deep learning model that is used for computer vision tasks, such as image classification and object detection. It is based on the Transformer architecture, originally developed for natural language processing tasks. The model works by encoding an image into a set of feature maps and then passing these maps through multiple self-attention layers. The self-attention mechanism helps the model to identify important features in the image and to learn the relationships between them. This allows the model to focus on specific regions of the image and to make predictions based on those features. The output of the self-attention layers is then fed into a feedforward network to make a final prediction. The model repeats this process multiple times to refine the predictions. This architecture allows the Vision Transformer to learn both global and local features in an image, making it well-suited for a variety of computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61103980",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Also, we need to look at some metrics to properly understand how good our model gets with each epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1469c7",
   "metadata": {},
   "source": [
    "#### ROUGE\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics for evaluating the quality of text summarization and machine translation outputs by comparing an automatic summary or translation to a reference or a set of references. The metrics calculate the overlap between n-grams, word sequences, or sentences of the automatic and reference summaries, giving scores for recall, precision, and the harmonic mean (F1 score). ROUGE is widely used in the evaluation of summarization and machine translation tasks in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bf372",
   "metadata": {},
   "source": [
    "#### METEOR\n",
    "\n",
    "METEOR (Metric for Evaluation of Translation with Explicit Ordering) is a metric for evaluating the quality of machine translation and text summarization outputs. It is similar to BLEU, but takes into account synonymy, stemming, and fluency in addition to n-gram overlap.\n",
    "\n",
    "$$ \\text{METEOR} = ( \\text{harmonic mean of precision and recall}) * (\\text{brevity penalty} + (1 - \\text{brevity penalty}) * \\text{harmonic mean of exact match and stem match})$$\n",
    "\n",
    "The precision and recall are calculated as the ratio of the number of correctly translated words to the total number of words in the generated and reference summaries, respectively. The brevity penalty is applied to adjust the score based on the length of the generated summary relative to the reference summary. If the generated summary is shorter than the reference summary, the brevity penalty is applied to lower the METEOR score.\n",
    "\n",
    "The exact match component of the METEOR score measures the overlap between the generated and reference summaries in terms of exact word matches. The stem match component measures the overlap between the generated and reference summaries in terms of word stems, taking into account synonymy.\n",
    "\n",
    "In summary, METEOR provides a more nuanced measure of the quality of machine translation and text summarization outputs than BLEU, as it takes into account factors such as synonymy and fluency in addition to n-gram overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7d581",
   "metadata": {},
   "source": [
    "#### BLEU\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is a commonly used metric for evaluating the quality of machine translation and text summarization outputs. It calculates the n-gram overlap between the generated output and a reference or a set of references. The score is based on the idea that the closer the generated output is to the reference, the better the quality of the output.\n",
    "\n",
    "The formula for BLEU is as follows:\n",
    "\n",
    "$$ \\text{BLEU} = exp(sum(log(p_i))) * \\text{BP} $$\n",
    "\n",
    "where $p_i$ is the modified precision for each n-gram order (1 to 4), $\\text{BP}$ is the brevity penalty, and $exp$ is the exponential function. The modified precision is calculated as the ratio of the number of correctly translated n-grams in the generated output to the total number of n-grams in the generated output, clipped to the number of corresponding n-grams in the reference(s).\n",
    "\n",
    "The brevity penalty is used to adjust the BLEU score based on the length of the generated output relative to the reference(s). If the generated output is shorter than the reference(s), the brevity penalty is applied to lower the BLEU score.\n",
    "\n",
    "BLEU scores range from 0 to 1, with higher scores indicating better quality outputs. The score provides a quick and simple way to evaluate the quality of machine translation and text summarization outputs, but it has some limitations, such as not taking into account synonymy or fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a59c5",
   "metadata": {},
   "source": [
    "### Beam search\n",
    "Beam search is a heuristic search algorithm used in natural language processing and computer vision to find the most likely sequence of elements in a large search space. It works by maintaining a fixed number of \"beams\" or candidate sequences, at each step of the search, the beams are expanded by adding the next element to each beam and scoring the resulting sequence, then the beams are pruned, keeping only the top N highest-scoring beams, where N is the width of the beam. The algorithm continues until a stopping criterion is met, such as finding an end-of-sequence symbol or reaching a maximum number of steps. Beam search is a trade-off between the completeness of an exhaustive search and the efficiency of greedy search, as it balances the exploration of the search space and exploitation of the highest-scoring sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe6145",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415ff79",
   "metadata": {},
   "source": [
    "First, we will load the captions dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83474f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/captions.json', 'r') as captions_file:\n",
    "    caption_dict = json.load(captions_file)\n",
    "    del captions_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b94e9a",
   "metadata": {},
   "source": [
    "Then we will change the paths to the directory since the original paths are `/content/filename.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = './data/Images/'\n",
    "images = list(caption_dict.keys())\n",
    "\n",
    "for image_path in images:\n",
    "    if image_path.endswith('jpg'):\n",
    "        new = images_path + image_path.split('/')[-1]\n",
    "        caption_dict[new] = caption_dict.pop(image_path)\n",
    "    else:\n",
    "        caption_dict.pop(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63d6af",
   "metadata": {},
   "source": [
    "Now, we will do train/test split on our captions because we need eval dataset for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5dc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dictionary, test_size_frac):\n",
    "    images = dictionary.keys()\n",
    "    images_test = random.sample(images,int(test_size_frac*len(images)))\n",
    "    images_train = [img for img in images if img not in images_test]\n",
    "\n",
    "    train_dict = {\n",
    "      img: dictionary[img] for img in images_train\n",
    "    }\n",
    "\n",
    "    test_dict = {\n",
    "      img: dictionary[img] for img in images_test\n",
    "    }\n",
    "    return(train_dict,test_dict)\n",
    "\n",
    "train,test = train_test_split(caption_dict, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb8b170",
   "metadata": {},
   "source": [
    "We will now create a pandas DataFrame because later we will create a PyTorch dataset in which we will do feature extraction on images and tokenization on texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c24514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dictionary):\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    captions = []\n",
    "    images = []\n",
    "    for image in list(caption_dict.keys()):\n",
    "        caption = caption_dict[image]\n",
    "        for capt in caption:\n",
    "            captions.append(' '.join(capt.replace('<s> ','').replace('  <e>','').strip().split(' ')[:30]))\n",
    "            images.append(image)\n",
    "\n",
    "    df['images'] = images\n",
    "    df['captions'] = captions\n",
    "    return(df)\n",
    "\n",
    "train_df = get_df(train)\n",
    "test_df = get_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ec709",
   "metadata": {},
   "source": [
    "## Preparing and Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d94ba",
   "metadata": {},
   "source": [
    "In this section we will prepare and train our image captioning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1915d",
   "metadata": {},
   "source": [
    "First, we need to load tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb397421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('./models/Byte_tokenizer', max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2241a7",
   "metadata": {},
   "source": [
    "Then we will load our feature_extractor for use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a095bf",
   "metadata": {},
   "source": [
    "Now, we will create a class for PyTorch dataset in which every time we get an item, its image is preprocessed using the feature extractor and each caption is tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer,feature_extractor, decoder_max_length=31):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.decoder_max_length = decoder_max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        img_path = self.df['images'][idx]\n",
    "        caption = self.df['captions'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.tokenizer(caption, truncation = True,\n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.decoder_max_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n",
    "    \n",
    "train_dataset = ImageCaptioningDataset(df=train_df,\n",
    "                           tokenizer=tokenizer,\n",
    "                          feature_extractor= feature_extractor)\n",
    "eval_dataset = ImageCaptioningDataset(df=test_df,\n",
    "                           tokenizer=tokenizer,feature_extractor= feature_extractor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a8de0",
   "metadata": {},
   "source": [
    "Now we create our model, its pretty simple. The parameter `tie_encoder_decoder` is used to create a cross attention head connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a35020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set encoder decoder tying to True\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained\\\n",
    "                    (\"google/vit-base-patch16-224\", './models/RobertaDecoder/', tie_encoder_decoder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f731d",
   "metadata": {},
   "source": [
    "Now, we need to configure it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b900ff",
   "metadata": {},
   "source": [
    "First, we need to set token-related properties. Here is the list:\n",
    "\n",
    "* decoder_start_token_id - The ID of the decoder start token\n",
    "* pad_token_id - The ID of the pad token\n",
    "* eos_token_id - The id of the end of sequence token\n",
    "* vocab_size - The size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd918b6d",
   "metadata": {},
   "source": [
    "The we need to configure parameters for beam search. Here is the list:\n",
    "\n",
    "* max_length - Maximum sequence length\n",
    "* no_repeat_ngram_size - The ngram size. Used to ensure that all words in an ngram are unique.\n",
    "* length_penalty - The length penalty is a scalar value that is applied to the final score of each candidate sequence, based on its length. The idea behind this technique is to give longer sequences a lower score, in order to encourage the generation of shorter outputs.\n",
    "* num_beams - Number of beams in beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a879ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_length = MAX_LEN\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9061f81",
   "metadata": {},
   "source": [
    "After configuring model, we need to define a function that computes all metrics we want to use for checking model quality. In our case we use ROUGE, METEOR and BLEU. The function will decode texts and use the objects loaded from `evaluate` library to compute the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "    meteor_output = meteor.compute(predictions=pred_str, references=label_str)\n",
    "    bleu_output = bleu.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    metrics_output = rouge_output\n",
    "    metrics_output.update({\"meteor\": meteor_output[\"meteor\"], \"bleu\": bleu_output[\"bleu\"]})\n",
    "\n",
    "    return metrics_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68016ab4",
   "metadata": {},
   "source": [
    "## Training Image Captioning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68d627",
   "metadata": {},
   "source": [
    "In this section we will train our model. We already know all of the parameters from the previous notebook so I do not feel need to explain them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "captioning_model = './models/Untrained_VIT_Captioning'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=captioning_model,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=2048,\n",
    "    num_train_epochs = TRAIN_EPOCHS,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    tokenizer=feature_extractor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5cca5a",
   "metadata": {},
   "source": [
    "And we start training. \n",
    "\n",
    "Important Warning: The model uses around 4 gigabytes of gpu memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e205c5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model, training and evaluating on the train dataset\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1bbd5",
   "metadata": {},
   "source": [
    "Let's save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./models/Trained_VIT_Captioning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300736d3",
   "metadata": {},
   "source": [
    "## Loading and Evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfa1e9",
   "metadata": {},
   "source": [
    "In this section we will test it on some images from test set. You can also use your own images, just call our captioning function with your own path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a0032",
   "metadata": {},
   "source": [
    "First, we will load our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94454c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained('./models/Trained_VIT_Captioning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f587194",
   "metadata": {},
   "source": [
    "Then we need to create the captioning function. It will open image, apply feature extraction, generate caption, decode it, replace special tokens and remove multiple spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90eba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image(image_path):\n",
    "    opened_image = Image.open(image_path).convert(\"RGB\")\n",
    "    feature_extraction = feature_extractor(opened_image, return_tensors=\"pt\").pixel_values\n",
    "    generated_caption = model.generate(feature_extraction)[0]\n",
    "    decoded_caption = tokenizer.decode(generated_caption)\n",
    "    \n",
    "    decoded_caption = decoded_caption.replace(\"<s>\", \"\")\n",
    "    decoded_caption = decoded_caption.replace(\"</s>\", \"\")\n",
    "    decoded_caption = re.sub(\"\\s+\", \" \", decoded_caption)\n",
    "\n",
    "    return decoded_caption.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047938eb",
   "metadata": {},
   "source": [
    "Then, we will get a random image from test set and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_path_from_test_set = test_df.sample(1).images.iloc[0]\n",
    "Image.open(temp).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d34f83",
   "metadata": {},
   "source": [
    "Now, we can use our function to caption a random image from the test set, and for example, an image of a domino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e721b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_image(random_path_from_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980cfb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_image(\"Dominoes.webp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533f063",
   "metadata": {},
   "source": [
    "It probably will not generate a good caption for the domino image, since it has never seen any domino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d3504",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In these two notebook we passed through the required steps for creating image captioning model with HuggingFace and learned a bunch of new things (probably). I hope you liked this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c686946",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13dfe8",
   "metadata": {},
   "source": [
    "* https://medium.com/@kalpeshmulye/image-captioning-using-hugging-face-vision-encoder-decoder-step-2-step-guide-part-1-495ecb05f0d5\n",
    "\n",
    "* https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/trainer#trainer\n",
    "\n",
    "* https://huggingface.co/docs/transformers/v4.26.0/en/model_doc/roberta\n",
    "\n",
    "* https://huggingface.co/docs/transformers/v4.26.0/en/model_doc/vit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
